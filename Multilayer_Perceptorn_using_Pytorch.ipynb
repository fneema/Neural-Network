{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer perceptron using Pytorch\n",
    "\n",
    "This notebook shows how to create a multilayer perceptron using Pytorch. First, we will see what is a multilayer perceptron and then we will go through the steps of building a multilayer perceptron in a simple but clear way. The objective is to build a multilayer perceptron to make predictions.\n",
    "\n",
    "**What is a multilayer perceptron?**\n",
    "\n",
    "A multilayer perceptron is a feedforward neural network which consists of atleast three layers, that is; an input layer, hidden layer and an output layer. We can have more than one hidden layer in between the input and output layer of a multilayer perceptron. In a feedforward neural network, data move in a forward direction only, that is, from the input to the output layer. Each layer consists of nodes called neurons, which are also known as units. The neurons in each layer use linear or non linear activation functions to produce an output. This output is fed into the immediate neuron as an input. The non linear activation starts at the immediate layer of the input layer.  \n",
    "We use multilayer perceptron to make predictions, recognitions and classification of data. Multilayer perceptrons have the ability to learn non-linearity patterns of data. The diagrams below show examples of a multilayer perceptron.The first diagram is a multilayer perceptron with one hidden layer while the second diagram is a multilayer pereptron with two hidden layers. Diagram source: https://www.learnopencv.com/wp-content/uploads/2017/10/mlp-mnist-schematic.jpg, https://becominghuman.ai/multi-layer-perceptron-mlp-models-on-real-world-banking-data-f6dd3d7e998f.  From the diagrams, we can see that the MLPs are fully connected, each node in one layer connects to every node in the immediate layer. This implies that each node in one layer connects with a certain weight to every node in the immediate layer. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1. <img src=\"MLP.png\" alt=\"Drawing\" style=\"width: 300px;\" /> \n",
    "\n",
    "2.  <img src=\"mm.png\" alt=\"Drawing\" style=\"width: 300px;\"/> \n",
    "\n",
    "**Steps in buliding a multilayer perceptron (MLP)**\n",
    "\n",
    "1. **Importing required libraries**\n",
    "\n",
    "As mentioned earlier that we will build the MLP using pytorch, the first step is to import this library. Pytorch is library used for deep learning using GPUs and CPUs. It uses tensors as opposed to arrays used by numpy library. Pytorch has several API, that is, functions that performs different tasks. For example, the **torch.utils.data.DataLoader** function that makes dataset to be iterable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Loading dataset**\n",
    "\n",
    "The second step after importing the necessary libraries, is to load your data. The data that was used here is the CIFAR10 data obtained from **torchvision**, which is a package in pytorch consisting of several datasets. To access and load our dataset, we use the function, **torchvision.datasets**. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dsets.CIFAR10('path/to/CIFAR10_root/', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "test_dataset = dsets.CIFAR10('path/to/CIFAR10_root/', \n",
    "                            train=False, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Making dataset iterable**\n",
    "\n",
    "In step three, we will use the function, **DataLoader**, from the torchvision package to make our dataset iterable. At this step will initialize the batch size of our dataset that will be used in the training of our model. We can also shuffle our data in this step if necessary. The batch-size used here is 200, one can take any amount of batch size. We set shuffle to be true inorder to shuffle our data. Here, we can also initialize the number of iterations and epochs which are used in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "no_iterations = 6000\n",
    "no_epochs = no_iterations/(len(train_dataset)/batch_size)\n",
    "no_epochs = int(no_epochs)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Creating MLP model class**\n",
    "\n",
    "Now, it is time to create our MLP model class. Our class will contain two parts; initialization part and defining the forward pass process. The forward pass involves two step; application of linear and non linear function. Our MLP neural network will have 3 hidden layers and two activation function will be used; **Rectified Linear Unit and tanh function**. **nn.Module** defines the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, no_neurons,output_dim):\n",
    "        super(MLP,self).__init__()\n",
    "        ###initialization step##\n",
    "        self.fc1 = nn.Linear(input_dim, no_neurons)\n",
    "        self.non_linear1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(no_neurons, no_neurons)\n",
    "        self.non_linear2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(no_neurons, no_neurons)\n",
    "        self.non_linear3 = nn.Tanh()\n",
    "        self.fc4 = nn.Linear(no_neurons, output_dim)\n",
    "    def forward(self,x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.non_linear1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.non_linear2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.non_linear3(out)\n",
    "        out = self.fc4(out)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instantiating model class**\n",
    "\n",
    "Here we specify the size of our image, which is the input dimension. CIFA10 has image with width size of 32, length size of 32 and 3 channels. \n",
    "We specify the number of neurons/units in the first layer, this can be any number.\n",
    "We specify the number of output, depending on the number of labels or categories you have in your target variable. CIFAR10 has 10 labels, so our output dimension will be 10.\n",
    "We also call our model calss here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 32*32*3\n",
    "no_neurons = 500\n",
    "output_dim = 10\n",
    "model = MLP(input_dim, no_neurons, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instantiating loss function**\n",
    "\n",
    "We call the loss function from the nn.module. Here we use CrossEntropyLoss since we are predicting classes. The loss function evaluates our model, whether it preforms well or poor. It calculates the difference beteen the predicted output and the actual output. If the error is too big, then this implies that our model performs poorly. This is a simple explanation but there is a deep explanation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instantiating optimizer function**\n",
    "\n",
    "We call the Stochastic Gradient Descent (SGD) from the torch library which  calculates weights. We set a learning rate which determines the size of the steps to be taken in gradient descent. Too big learning rate, may lead to divergence of the gradient descent, the gradient descent may not converge and for too small learning rate, the gradient descent may take long to converge. Therefore, the learning rate should sufficiently chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training and testing our model**\n",
    "\n",
    "This is the final part, in which we now train our model and test it. We call all the functions defined above and make iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 1.7275830507278442. Accuracy: 37.130001068115234\n",
      "Iteration: 1000. Loss: 1.6027252674102783. Accuracy: 39.619998931884766\n",
      "Iteration: 1500. Loss: 1.5468699932098389. Accuracy: 45.63999938964844\n",
      "Iteration: 2000. Loss: 1.43499755859375. Accuracy: 46.90999984741211\n",
      "Iteration: 2500. Loss: 1.5316741466522217. Accuracy: 46.07999801635742\n",
      "Iteration: 3000. Loss: 1.4730250835418701. Accuracy: 45.12000274658203\n",
      "Iteration: 3500. Loss: 1.3327068090438843. Accuracy: 49.95000076293945\n",
      "Iteration: 4000. Loss: 1.4402254819869995. Accuracy: 50.790000915527344\n",
      "Iteration: 4500. Loss: 1.4370384216308594. Accuracy: 47.77000045776367\n",
      "Iteration: 5000. Loss: 1.251592993736267. Accuracy: 50.55999755859375\n",
      "Iteration: 5500. Loss: 1.1726099252700806. Accuracy: 52.1099967956543\n",
      "Iteration: 6000. Loss: 1.2683820724487305. Accuracy: 50.660003662109375\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Iter = 0\n",
    "for epoch in range(no_epochs):\n",
    "    #Enumerate assigns index to each item in an iterable object\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        #Load images\n",
    "        images = images.view(-1,32*32*3).requires_grad_()\n",
    "        \n",
    "        \n",
    "        #Clear grad with respect to parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Forward pass to get outputs\n",
    "        outputs = model(images)\n",
    "        \n",
    "        #Calculate loss\n",
    "        loss = criterion(outputs,labels)\n",
    "        #backward pass\n",
    "        \n",
    "        loss.backward()\n",
    "        #updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        Iter += 1\n",
    "        \n",
    "        if Iter % 500 == 0:\n",
    "            #calculate accuracy\n",
    "            correct = 0\n",
    "            \n",
    "            total = 0\n",
    "            #iterate through test set\n",
    "            for images, labels in test_loader:\n",
    "                #load images\n",
    "                images = images.view(-1,32*32*3).requires_grad_()\n",
    "                \n",
    "                #Forward_pass to get output\n",
    "                outputs = model(images)\n",
    "                \n",
    "                #Get prediction sfrom maximum value\n",
    "                _, predicted = torch.max(outputs.data,1)\n",
    "                \n",
    "                #total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                #total correct predictions\n",
    "\n",
    "                correct += (predicted == labels).sum()\n",
    "            \n",
    "            accuracy =  (correct / float(total)) * 100\n",
    "\n",
    "            \n",
    "           \n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(Iter, loss.item(), accuracy))       \n",
    "            \n",
    "            #torch.save(model, path)\n",
    "            #torch.save(model.state_dict(),path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "Finally, we have our accuaracy results which is 50.66%, which is not good accuracy. There are some other ways that we can consider, like fine-tuning the hyperparameters and also do some regularization inorder to improve accuracy. At this point, we have achieved our objective which was to build a multilayer perceptron using pytorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
