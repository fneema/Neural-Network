{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network\n",
    " A convolutional neural network (CNN, or ConvNet) is a class of deep neural networks, most commonly applied to analyze visual images.\n",
    "One way that might improve results in fully conneceted layers is by adding more hidden layers or making the network to be deep. But this can cause problems such as vanishing gradient. In order to fix this problem in a model, the ReLU activation functions can be used. Another problem that is associted with deep fully connected networks is that the trainable parameters (weights) in the model can grow rapidly. This might result to slow training or the training can never happen, and also might lead to overfitting of the model. We can solve this problem by using the convolution neural network. How??\n",
    "\n",
    "Convolution neural network exploits the correlation between adjecent inputs in the images. For example, It is likely that the pixels aclose to the cat's eyes are more likely to be connected with nearby pixels which show the cats nose-rather than the pixels on the other side of the image that represents the dog's nose. Therefore, not every other node in the network needs to be connecetd to every other node in the next layer. This helps in reducng the number of weights or parameters required to be trained in the model.\n",
    "\n",
    "<img src=\"cnn1.png\">\n",
    "\n",
    "[Diagram source](https://www.udemy.com/course/deep-learning-machine-learning-practical/)\n",
    "\n",
    "\n",
    "**How does the CNN works?**\n",
    "\n",
    "The convolution neural network is composed of the following layers in which each layer perform specific tasks:\n",
    "\n",
    "**Convolution layer**\n",
    "\n",
    "- Uses image kernel matrix also kwon as filter.\n",
    "- It performs convolution operation (dot product of the original pixel values with the weights defined in the filter.\n",
    "- The kernel is scanned over a given image and does feauture extraction to select most important pixels of that image.\n",
    "- The kernel apply effects such as blurring and sharpening.\n",
    "\n",
    "**How does convolutional operation help?**\n",
    "\n",
    "The convolutional neural network reduce the number of parameters which need to be trained by using the following properties.\n",
    "1. It provides sparse connections – not every node in the first / input layer is connected to every node in the second layer. This is contrary to fully connected neural networks, where every node is connected to every other in the following layer.\n",
    "2.   Constant filter parameters – each filter has constant parameters. In other words, as the filter moves around the image, the same weights are applied to each 2 x 2 set of nodes. Each filter, as such, can be trained to perform a certain specific transformation of the input space. Therefore, each filter has a certain set of weights that are applied for each convolution operation – this reduces the number of parameters.\n",
    "\n",
    "<img src=\"conv.png\">\n",
    "[Diagram source](https://www.udemy.com/course/deep-learning-machine-learning-practical/)\n",
    "\n",
    "**Activation layer**\n",
    "- The activation function that is used in CNN is the non linear RELU activation function.\n",
    "- ReLU is used to add non-linearity in the feature map.\n",
    "- It enhances the sparsity or how scattered the feature map is.\n",
    "- The advantage of ReLU is that, its gradinet does not vanish as we increase the input as compared to sigmoid function.\n",
    "- ReLU function is represented as; f(x)= max(0,x), this implies that, it takes in input x and return x if x is positive and returns 0 if x is negative.\n",
    "\n",
    "The following is an illustraion of how ReLU function works.\n",
    "\n",
    "<img src=\"Relu.png\">\n",
    "\n",
    "[Diagram source](https://www.udemy.com/course/deep-learning-machine-learning-practical/)\n",
    "\n",
    "**Pooling or down-sampling layer**\n",
    "- This layer reduces the dimensionality or size of the feature map or the number of trainable parameters in the model.\n",
    "- This helps is improving the computational efficency by allowing the network to train much faster, focusing on the most importatnt information.\n",
    "- It also helps in generalizing the model hence avoiding overfitting.\n",
    "- There various kinds of pooling but the nost commonly used is the maximum pooling.\n",
    "- The maximum pooling works by retaining the maximum value in a given sample size of a feature map.\n",
    "\n",
    "The following is a pooling diagram which illustrate how maximum pooling works.\n",
    "\n",
    "<img src=\"Pool.png\">\n",
    "\n",
    "[Diagram source](https://www.udemy.com/course/deep-learning-machine-learning-practical/)\n",
    "\n",
    "- Strides and down-sampling\n",
    "\n",
    "In the pooling diagram above, we can see that the pooling window in blue color which supposed to be shifted to the right each time by 2 places, both in x and y direction. In other words, the stride is specified as [2, 2]. One important thing to notice is that, if during pooling the stride is greater than 1, then the output size will be reduced. As can be observed above, the 4 x 4 input is reduced to a 2 x 2 output. This is a good thing – it is called down-sampling, and it reduces the number of trainable parameters in the model.\n",
    "\n",
    "- Padding\n",
    "\n",
    "Padding is adding an extra column and row to the input image for example the 4 $\\times$ 4 input, this helps is giving the effective size of the pooling space. It helps in ensuring that the pooling windo can correctly operate with a given stride.  Padding may or may not be considered when constructing a convolutional neural network in PyTorch.\n",
    "\n",
    "\n",
    "**Fully connected layer**\n",
    "- The last layer in a CNN is the fully connected layer. \n",
    "- This layer performs interpretation of the results and gives an output of the classified result. \n",
    "- In order to attach this fully connected layer to the network, the dimensions of the output of the CNN need to be flattened.\n",
    "\n",
    "The diagram below show a flatten pooled matrix and the fully connected layer attached to the CNN.\n",
    "\n",
    "\n",
    "<img src=\"flat.png\">\n",
    "\n",
    "[Diagram source](https://www.udemy.com/course/deep-learning-machine-learning-practical/)\n",
    "\n",
    "**How to improve accuracy of a CNN?**\n",
    "\n",
    "- The accuary of the model can be improved by increasing the number of filters or by adding a dropout.\n",
    "- Dropout refers to dropping out units/neurons in the network.\n",
    "- We drop the neurons because if they are too many they might develop co-dependency amongst each other during training.\n",
    "- Dropout is a regularisation technique for reducing overfitting in neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementing CNN using pytorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import PIL\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import skimage.transform\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from IPython import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def show(img):\n",
    "    \"\"\"Show PyTorch tensor img as an image in matplotlib.\"\"\"\n",
    "    npimg = img.cpu().detach().numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
    "    plt.grid(False)\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "def display_thumb(img):\n",
    "  display.display(transforms.Resize(128)(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training and test dataset.\n",
    "mnist_train = datasets.MNIST('/tmp/mnist', train=True, download=True)\n",
    "mnist_test = datasets.MNIST('/tmp/mnist', train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label of image: 5 - five\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABAElEQVR4nGNgGMyAWUhIqK5jvdSy/9/rGRgYGFhgEnJsVjYCwQwMDAxPJgV+vniQgYGBgREqZ7iXH8r6l/SV4dn7m8gmCt3++/fv37/Htn3/iMW+gDnZf/+e5WbQnoXNNXyMs/5GoQoxwVmf/n9kSGFiwAW49/11wynJoPzx4YIcRlyygR/+/i2XxCWru+vv32nSuGQFYv/83Y3b4p9/fzpAmSyoMnohpiwM1w5h06Q+5enfv39/bcMiJVF09+/fv39P+mFKiTtd/fv3799jgZiBJLT69t+/f/8eDuDEkDJf8+jv379/v7Ryo4qzMDAwMAQGMjBc3/y35wM2V1IfAABFF16Aa0wAOwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FBEFF82AD30>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show a random image and the corresponding target.\n",
    "img, target = mnist_train[0]\n",
    "print('Label of image:', mnist_train.classes[target])\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This ensures the MNIST dataset produces PyTorch tensors.\n",
    "mnist_train.transform = transforms.ToTensor()\n",
    "mnist_test.transform = transforms.ToTensor()\n",
    "\n",
    "# Size of the batches the data loader will produce.\n",
    "batch_size = 64\n",
    "\n",
    "# This creates the dataloaders.\n",
    "train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalNetwork(nn.Module):\n",
    "  \"\"\"Simple convolutional network.\"\"\"\n",
    "  \n",
    "  def __init__(self, input_size, num_classes, in_channels=1, num_neurons=50):\n",
    "      super(ConvolutionalNetwork, self).__init__()\n",
    "      \n",
    "      self.conv_network1 = nn.Sequential(nn.Conv2d(in_channels, num_neurons,\n",
    "                                kernel_size=5),nn.ReLU(),nn.MaxPool2d(kernel_size=2,stride=2))\n",
    "                                        \n",
    "        \n",
    "                        \n",
    "      self.conv_network2 = nn.Sequential(nn.Conv2d( num_neurons, 2*num_neurons,\n",
    "                                kernel_size=5),nn.ReLU(),nn.MaxPool2d(kernel_size=2,stride=2))\n",
    "                                        \n",
    "      self.drop_out = nn.Dropout()                          \n",
    "      self.linear1= nn.Linear(2*num_neurons*4*4,50)\n",
    "      self.linear2 = nn.Linear(50,num_classes)\n",
    "      \n",
    "  def forward(self, x):\n",
    "\n",
    "      x = self.conv_network1(x)\n",
    "      x = self.conv_network2(x)\n",
    "      \n",
    "      x = x.view(x.size(0), -1)\n",
    "      x = self.drop_out(x)\n",
    "      x = self.linear1(x)\n",
    "      x = self.linear2(x)\n",
    "      return x\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, data_loader, optimizer, num_epochs):\n",
    "    \"\"\"Simple training loop for a PyTorch model.\"\"\"\n",
    "    \n",
    "    # Make sure model is in training mode.\n",
    "    model.train()\n",
    "    # Move model to the device.\n",
    "    model.to(device)\n",
    "    \n",
    "    # Exponential moving average of the loss.\n",
    "    ema_loss = None\n",
    "    \n",
    "    # Loop over epochs.\n",
    "    for epoch in range(num_epochs):\n",
    "      \n",
    "      # Loop over data.\n",
    "      for batch_idx, (data, target) in enumerate(data_loader):\n",
    "          data = data.to(device)\n",
    "          target = target.to(device)\n",
    "        \n",
    "          # Forward pass.\n",
    "          output = model(data)\n",
    "          loss = criterion(output, target)\n",
    "          \n",
    "          # Backward pass.\n",
    "          optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "          \n",
    "          if ema_loss is None:\n",
    "            ema_loss = loss.item()\n",
    "          else:\n",
    "            ema_loss += (loss.item() - ema_loss) * 0.01 \n",
    "          \n",
    "          # Print out progress.\n",
    "          if batch_idx % 500 == 0:\n",
    "              print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), \n",
    "                    len(data_loader.dataset),\n",
    "                    100. * batch_idx / len(data_loader), ema_loss),\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, data_loader):\n",
    "    \"\"\"Measures the accuracy of a model on a data set.\"\"\"\n",
    "    \n",
    "    # Make sure the model is in evaluation mode.\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "\n",
    "    # We do not need to maintain intermediate activations while testing.\n",
    "    with torch.no_grad():\n",
    "      \n",
    "        # Loop over test data.\n",
    "        for data, target in data_loader:\n",
    "          \n",
    "            # Forward pass.\n",
    "            output = model(data.to(device))\n",
    "            \n",
    "            # Get the label corresponding to the highest predicted probability.\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            \n",
    "            # Count number of correct predictions.\n",
    "            correct += pred.cpu().eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    # Print test accuracy.\n",
    "    print('Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "          correct, \n",
    "          len(data_loader.dataset),\n",
    "          100. * correct / len(data_loader.dataset)),\n",
    "    )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.329809\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.747553\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.348115\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.246460\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.195598\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.163108\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.158312\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.135728\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.119801\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.112035\n",
      "Accuracy: 9768/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create and train convolutional network.\n",
    "conv_model = ConvolutionalNetwork(28, 10)\n",
    "\n",
    "#criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Make sure you optimize conv_model rather than model.\n",
    "optimizer =torch.optim.SGD(conv_model.parameters(), lr=0.01)\n",
    "\n",
    "#training the model\n",
    "train(conv_model, criterion, train_loader, optimizer, num_epochs=5)\n",
    "\n",
    "#testing the model\n",
    "test(conv_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
